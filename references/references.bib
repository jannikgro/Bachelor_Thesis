@inproceedings{firstDemoReference,
  author    = {My Name and A Co-Author},
  title     = {Useless Stuff That No One Cares About},
  booktitle = {\bibstoc{coolest}{XX}},
  year      = {2025},
  pages     = {42--1337},
  publisher = ACM,
  numpages  = {1296}
}

@article{secondDemoReference,
  author    = {My Name and First Co-Author and Second Co-Author and Third Co-Author and Fourth Co-Author},
  title     = {Dear Lord! How Did This Get Accepted?},
  journal   = ZMLGM,
  year      = {2030},
  volume    = {42},
  number    = {1},
  pages     = {2--1024},
  publisher = {No Clue}
}

@article{DBLP:journals/corr/abs-1712-01815,
  author     = {David Silver and
                Thomas Hubert and
                Julian Schrittwieser and
                Ioannis Antonoglou and
                Matthew Lai and
                Arthur Guez and
                Marc Lanctot and
                Laurent Sifre and
                Dharshan Kumaran and
                Thore Graepel and
                Timothy P. Lillicrap and
                Karen Simonyan and
                Demis Hassabis},
  title      = {Mastering Chess and Shogi by Self-Play with a General Reinforcement
                Learning Algorithm},
  journal    = {CoRR},
  volume     = {abs/1712.01815},
  year       = {2017},
  url        = {http://arxiv.org/abs/1712.01815},
  eprinttype = {arXiv},
  eprint     = {1712.01815},
  timestamp  = {Mon, 13 Aug 2018 16:46:01 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1712-01815.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@book{Sutton1998,
  added-at  = {2019-07-13T10:11:53.000+0200},
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  biburl    = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition   = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords  = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title     = {Reinforcement Learning: An Introduction},
  url       = {http://incompleteideas.net/book/the-book-2nd.html},
  year      = {2018 }
}

@book{deges2019grundlagen,
  title     = {Grundlagen des E-Commerce: Strategien, Modelle, Instrumente},
  author    = {Deges, F.},
  isbn      = {9783658263201},
  url       = {https://books.google.de/books?id=Fz\_ADwAAQBAJ},
  year      = {2019},
  publisher = {Springer Fachmedien Wiesbaden}
}

@inproceedings{10.5555/3044805.3044850,
  author    = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  title     = {Deterministic Policy Gradient Algorithms},
  year      = {2014},
  publisher = {JMLR.org},
  abstract  = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
  pages     = {I–387–I–395},
  location  = {Beijing, China},
  series    = {ICML'14}
}

@article{DBLP:journals/corr/abs-1802-09477,
  author     = {Scott Fujimoto and
                Herke van Hoof and
                David Meger},
  title      = {Addressing Function Approximation Error in Actor-Critic Methods},
  journal    = {CoRR},
  volume     = {abs/1802.09477},
  year       = {2018},
  url        = {http://arxiv.org/abs/1802.09477},
  eprinttype = {arXiv},
  eprint     = {1802.09477},
  timestamp  = {Sat, 28 Sep 2019 00:58:01 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1802-09477.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3219819.3219833,
  author    = {Schlosser, Rainer and Boissier, Martin},
  title     = {Dynamic Pricing under Competition on Online Marketplaces: A Data-Driven Approach},
  year      = {2018},
  isbn      = {9781450355520},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3219819.3219833},
  booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages     = {705–714},
  numpages  = {10},
  keywords  = {dynamic pricing, e-commerce, decision making, demand learning},
  location  = {London, United Kingdom},
  series    = {KDD '18}
}

@book{lapan2020deep,
  title     = {Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition},
  author    = {Lapan, M.},
  isbn      = {9781838820046},
  url       = {https://books.google.de/books?id=O0vODwAAQBAJ},
  year      = {2020},
  publisher = {Packt Publishing}
}

@incollection{NEURIPS2019_9015,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{adam2014,
  url       = {https://arxiv.org/abs/1412.6980},
  author    = {Kingma, Diederik P. and Ba, Jimmy},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Adam: A Method for Stochastic Optimization},
  publisher = {arXiv},
  year      = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{atari2014,
  doi       = {10.48550/ARXIV.1312.5602},
  url       = {https://arxiv.org/abs/1312.5602},
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Playing Atari with Deep Reinforcement Learning},
  publisher = {arXiv},
  year      = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{qlearningcomparison2017,
  doi       = {10.48550/ARXIV.1710.02298},
  url       = {https://arxiv.org/abs/1710.02298},
  author    = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  keywords  = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{qlearningoverestimation,
  doi       = {10.48550/ARXIV.1509.06461},
  url       = {https://arxiv.org/abs/1509.06461},
  author    = {van Hasselt, Hado and Guez, Arthur and Silver, David},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Deep Reinforcement Learning with Double Q-learning},
  publisher = {arXiv},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{10.1007/BF00992696,
  author     = {Williams, Ronald J.},
  title      = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  year       = {1992},
  issue_date = {May 1992},
  publisher  = {Kluwer Academic Publishers},
  address    = {USA},
  volume     = {8},
  number     = {3–4},
  issn       = {0885-6125},
  url        = {https://doi.org/10.1007/BF00992696},
  doi        = {10.1007/BF00992696},
  abstract   = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  journal    = {Mach. Learn.},
  month      = {may},
  pages      = {229–256},
  numpages   = {28},
  keywords   = {Reinforcement learning, mathematical analysis, connectionist networks, gradient descent}
}

@misc{trpo2015,
  doi       = {10.48550/ARXIV.1502.05477},
  url       = {https://arxiv.org/abs/1502.05477},
  author    = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Trust Region Policy Optimization},
  publisher = {arXiv},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ppo2018,
  doi       = {10.48550/ARXIV.1707.06347},
  url       = {https://arxiv.org/abs/1707.06347},
  author    = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Proximal Policy Optimization Algorithms},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{haarnoja2018soft,
  title         = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author        = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
  year          = {2018},
  eprint        = {1801.01290},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{stable-baselines,
  author       = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title        = {Stable Baselines},
  year         = {2018},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/hill-a/stable-baselines}}
}

@misc{https://doi.org/10.48550/arxiv.1506.02438,
  doi       = {10.48550/ARXIV.1506.02438},
  url       = {https://arxiv.org/abs/1506.02438},
  author    = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  keywords  = {Machine Learning (cs.LG), Robotics (cs.RO), Systems and Control (eess.SY), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title     = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  publisher = {arXiv},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1812.05905,
  doi       = {10.48550/ARXIV.1812.05905},
  url       = {https://arxiv.org/abs/1812.05905},
  author    = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Soft Actor-Critic Algorithms and Applications},
  publisher = {arXiv},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

﻿@article{Silver2017,
  author   = {Silver, David
              and Schrittwieser, Julian
              and Simonyan, Karen
              and Antonoglou, Ioannis
              and Huang, Aja
              and Guez, Arthur
              and Hubert, Thomas
              and Baker, Lucas
              and Lai, Matthew
              and Bolton, Adrian
              and Chen, Yutian
              and Lillicrap, Timothy
              and Hui, Fan
              and Sifre, Laurent
              and van den Driessche, George
              and Graepel, Thore
              and Hassabis, Demis},
  title    = {Mastering the game of Go without human knowledge},
  journal  = {Nature},
  year     = {2017},
  month    = {Oct},
  day      = {01},
  volume   = {550},
  number   = {7676},
  pages    = {354-359},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
  issn     = {1476-4687},
  doi      = {10.1038/nature24270},
  url      = {https://doi.org/10.1038/nature24270}
}

@misc{https://doi.org/10.48550/arxiv.1712.01815,
  doi       = {10.48550/ARXIV.1712.01815},
  url       = {https://arxiv.org/abs/1712.01815},
  author    = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  keywords  = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Gerpott2022,
  author   = {Gerpott, Torsten J.
              and Berends, Jan},
  title    = {Competitive pricing on online markets: a literature review},
  journal  = {Journal of Revenue and Pricing Management},
  year     = {2022},
  month    = {Jun},
  day      = {14},
  abstract = {Past reviews of studies concerning competitive pricing strategies lack a unifying approach to interdisciplinarily structure research across economics, marketing management, and operations. This academic void is especially unfortunate for online markets as they show much higher competitive dynamics compared to their offline counterparts. We review 132 articles on competitive posted goods pricing on either e-tail markets or markets in general. Our main contributions are (1) to develop an interdisciplinary framework structuring scholarly work on competitive pricing models and (2) to analyze in how far research on offline markets applies to online retail markets.},
  issn     = {1477-657X},
  doi      = {10.1057/s41272-022-00390-x},
  url      = {https://doi.org/10.1057/s41272-022-00390-x}
}

@article{Kastius2022,
  author   = {Kastius, Alexander
              and Schlosser, Rainer},
  title    = {Dynamic pricing under competition using reinforcement learning},
  journal  = {Journal of Revenue and Pricing Management},
  year     = {2022},
  month    = {Feb},
  day      = {01},
  volume   = {21},
  number   = {1},
  pages    = {50-63},
  abstract = {Dynamic pricing is considered a possibility to gain an advantage over competitors in modern online markets. The past advancements in Reinforcement Learning (RL) provided more capable algorithms that can be used to solve pricing problems. In this paper, we study the performance of Deep Q-Networks (DQN) and Soft Actor Critic (SAC) in different market models. We consider tractable duopoly settings, where optimal solutions derived by dynamic programming techniques can be used for verification, as well as oligopoly settings, which are usually intractable due to the curse of dimensionality. We find that both algorithms provide reasonable results, while SAC performs better than DQN. Moreover, we show that under certain conditions, RL algorithms can be forced into collusion by their competitors without direct communication.},
  issn     = {1477-657X},
  doi      = {10.1057/s41272-021-00285-3},
  url      = {https://doi.org/10.1057/s41272-021-00285-3}
}

@article{brockman2016openai,
  title   = {Openai gym},
  author  = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal = {arXiv preprint arXiv:1606.01540},
  year    = {2016}
}

@article{Kim2016DynamicPA,
  title   = {Dynamic Pricing and Energy Consumption Scheduling With Reinforcement Learning},
  author  = {Byung-Gook Kim and Yu Zhang and Mihaela van der Schaar and Jang-Won Lee},
  journal = {IEEE Transactions on Smart Grid},
  year    = {2016},
  volume  = {7},
  pages   = {2187-2198}
}

@article{RANA2015426,
  title    = {Dynamic pricing policies for interdependent perishable products or services using reinforcement learning},
  journal  = {Expert Systems with Applications},
  volume   = {42},
  number   = {1},
  pages    = {426-436},
  year     = {2015},
  issn     = {0957-4174},
  doi      = {https://doi.org/10.1016/j.eswa.2014.07.007},
  url      = {https://www.sciencedirect.com/science/article/pii/S095741741400400X},
  author   = {Rupal Rana and Fernando S. Oliveira},
  keywords = {Dynamic pricing, Reinforcement learning, Revenue management, Service management, Simulation},
  abstract = {Many businesses offer multiple products or services that are interdependent, in which the demand for one is often affected by the prices of others. This article considers a revenue management problem of multiple interdependent products, in which dynamically adjusted over a finite sales horizon to maximize expected revenue, given an initial inventory for each product. The main contribution of this article is to use reinforcement learning to model the optimal pricing of perishable interdependent products when demand is stochastic and its functional form unknown. We show that reinforcement learning can be used to price interdependent products. Moreover, we analyze the performance of the Q-learning with eligibility traces algorithm under different conditions. We illustrate our analysis with the pricing of services.}
}

@inproceedings{10.1007/978-3-030-87897-9_21,
  author    = {Alves, Julio C{\'e}sar
               and Silva, Diego Mello da
               and Mateus, Geraldo Robson},
  editor    = {Rutkowski, Leszek
               and Scherer, Rafa{\l}
               and Korytkowski, Marcin
               and Pedrycz, Witold
               and Tadeusiewicz, Ryszard
               and Zurada, Jacek M.},
  title     = {Applying and Comparing Policy Gradient Methods to Multi-echelon Supply Chains with Uncertain Demands and Lead Times},
  booktitle = {Artificial Intelligence and Soft Computing},
  year      = {2021},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {229--239},
  abstract  = {In the present work, we have applied and compared Deep Reinforcement Learning techniques to solve a problem usually addressed with Operations Research tools. State-of-the-art Policy Gradient methods are used in a production planning and product distribution problem, considering a four-echelon supply chain with uncertain lead times, to minimize total operating costs while meeting uncertain seasonal demands. Two-phases experiments are conducted regarding eight different scenarios. Firstly, A2C, DDPG, PPO, SAC, and TD3 are used considering fixed training budgets. In the second phase, the best two algorithms from the first phase are tuned considering different stopping criteria. Results show that PPO and SAC perform better for the problem addressed. They achieve comparable learning behavior, final performance, and sample complexity, while PPO is faster in terms of wall-clock time.},
  isbn      = {978-3-030-87897-9}
}

@article{KRASHENINNIKOVA20198,
  title    = {Reinforcement learning for pricing strategy optimization in the insurance industry},
  journal  = {Engineering Applications of Artificial Intelligence},
  volume   = {80},
  pages    = {8-19},
  year     = {2019},
  issn     = {0952-1976},
  doi      = {https://doi.org/10.1016/j.engappai.2019.01.010},
  url      = {https://www.sciencedirect.com/science/article/pii/S0952197619300107},
  author   = {Elena Krasheninnikova and Javier García and Roberto Maestre and Fernando Fernández},
  keywords = {Pricing strategy optimization, Reinforcement learning},
  abstract = {Pricing is a fundamental problem in the banking sector, and is closely related to a number of financial products such as credit scoring or insurance. In the insurance industry an important question arises, namely: how can insurance renewal prices be adjusted? Such an adjustment has two conflicting objectives. On the one hand, insurers are forced to retain existing customers, while on the other hand insurers are also forced to increase revenue. Intuitively, one might assume that revenue increases by offering high renewal prices, however this might also cause many customers to terminate their contracts. Contrarily, low renewal prices help retain most existing customers, but could negatively affect revenue. Therefore, adjusting renewal prices is a non-trivial problem for the insurance industry. In this paper, we propose a novel modelization of the renewal price adjustment problem as a sequential decision problem and, consequently, as a Markov decision process (MDP). In particular, this study analyzes two different strategies to carry out this adjustment. The first is about maximizing revenue analyzing the effect of this maximization on customer retention, while the second is about maximizing revenue subject to the client retention level not falling below a given threshold. The former case is related to MDPs with a single criterion to be optimized. The latter case is related to Constrained MDPs (CMDPs) with two criteria, where the first one is related to optimization, while the second is subject to a constraint. This paper also contributes with the resolution of these models by means of a model-free Reinforcement Learning algorithm. Results have been reported using real data from the insurance division of BBVA, one of the largest Spanish companies in the banking sector.}
}

@article{9086147,
  author  = {Cong, Peijin and Zhou, Junlong and Chen, Mingsong and Wei, Tongquan},
  journal = {IEEE Transactions on Cloud Computing},
  title   = {Personality-Guided Cloud Pricing via Reinforcement Learning},
  year    = {2022},
  volume  = {10},
  number  = {2},
  pages   = {925-943},
  doi     = {10.1109/TCC.2020.2992461}
}

@article{8356086,
  author  = {Mocanu, Elena and Mocanu, Decebal Constantin and Nguyen, Phuong H. and Liotta, Antonio and Webber, Michael E. and Gibescu, Madeleine and Slootweg, J. G.},
  journal = {IEEE Transactions on Smart Grid},
  title   = {On-Line Building Energy Optimization Using Deep Reinforcement Learning},
  year    = {2019},
  volume  = {10},
  number  = {4},
  pages   = {3698-3708},
  doi     = {10.1109/TSG.2018.2834219}
}

@article{Jose2020,
  author   = {Jose, Rajan
              and Panigrahi, Shrikant Krupasindhu
              and Patil, Rashmi Anoop
              and Fernando, Yudi
              and Ramakrishna, Seeram},
  title    = {Artificial Intelligence-Driven Circular Economy as a Key Enabler for Sustainable Energy Management},
  journal  = {Materials Circular Economy},
  year     = {2020},
  month    = {Sep},
  day      = {12},
  volume   = {2},
  number   = {1},
  pages    = {8},
  abstract = {Nearly a billion new energy consumers join the society in 13--15 years; and the growing demand for higher standards of living makes the worldwide energy consumption continuously growing. Strategic solutions are therefore required not only for addressing energy gaps but also for eliminating the undesirable environmental effects of energy supply chain to ensure quality and sustainable living. This article advocates leveraging artificial intelligence associated digital technologies to increase energy efficiency, to facilitate carbon trading, and to realize the circular economy vision of countries to mitigate extreme weather conditions and climate change.},
  issn     = {2524-8154},
  doi      = {10.1007/s42824-020-00009-9},
  url      = {https://doi.org/10.1007/s42824-020-00009-9}
}

@article{LarsenVessel,
  author  = {Larsen, Thomas and Teigen, Halvor and Laache, Torkel and Varagnolo, Damiano and Rasheed, Adil},
  year    = {2021},
  month   = {09},
  pages   = {},
  title   = {Comparing Deep Reinforcement Learning Algorithms’ Ability to Safely Navigate Challenging Waters},
  volume  = {8},
  journal = {Frontiers in Robotics and AI},
  doi     = {10.3389/frobt.2021.738113}
}

@online{RecommerceDefinition,
  author = {New York Times (Archived)},
  title  = {As I.T. Goes, So Goes Forrester?},
  year   = 2005,
  url    = {https://web.archive.org/web/20180613040854/https://www.nytimes.com/2005/02/18/business/yourmoney/as-it-goes-so-goes-forrester.html},
  note   = {Accessed: 2022-07-05}
}