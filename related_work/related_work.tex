Während Pricing ein altes Problem ist, war die Forschungsaktivität zu Pricing mit intensivem Wettbewerb und gerade auf Online-Marktplätzen bis vor einigen Jahren auf einem geringen Level, wie eine Übersichtsarbeit aus dem Jahre 2022 zeigt. \cite{Gerpott2022}
Demnach dominierten Monpolszenarien oder Szenarien mit starken, aber einschränkenden Annahmen.

Im Bereich der algorithmen- und datengetriebenen Preisoptimierung lässt sich die Schwierigkeit in zwei separate Herausforderungen unterteilen: Das Schätzen des Kundenverhaltens und das Optimieren des Pricings bei bekanntem oder erlerntem Kundenverhalten.
Schlosser und Boissier \cite{10.1145/3219819.3219833} lösen diese Probleme einzeln.
Zunächst werden Regressionsverfahren angewendet, um das Kundenverhalten zu erlernen und anschließend ein approximiertes Dynamic Programming Verfahren vorgestellt, um trotz des >>Fluchs der Dimension<< Preisstrategien optimieren zu können.
Die mit diesem Verfahren optimierten Preisstrategien konnten erfolgreich auf Amazon Marketplace angewandt werden.

Reinforcement Learning für dynamisches Pricing wurde in den letzten Jahren für spezielle Märkte angewandt, etwa den Energiemarkt bei Byung-Gook Kim et al. \cite{Kim2016DynamicPA}
Dort bestand die Aufgabe darin, mittels dynamischer Preise die Nachfrage von Energieverbrauchern gegenüber den Produzenten zu regulieren.
Es wurden Q-Learning-Verfahren angewandt und spezielle Anpassungen verglichen.
Dabei wurde auch ein Multi-Agent-Setup vorgestellt, das erheblich bessere Ergebnisse als reines Q-Learning lieferte.

Rana und Oliveira \cite{RANA2015426} verwenden Reinforcement Learning, um Preisstrategien für einen Markt zu optimieren, bei dem verderbliche Ware innerhalb eines endlichen Horizonts verkauft werden muss.
Dabei wird nicht nur eine Produktlinie, sondern sehr viele betrachtet.
Zwischen dem Verkauf unterschiedlicher Produktlinien gibt es komplexe Abhängigkeiten, die in klassischen Pricingansätzen nicht beachtet wurden, aber mit Reinforcement Learning mit einbezogen werden können.
Als Technologie wird Q-Learning mit Eligibility Traces verwendet.

Weiterhin haben Krasheninnikova et al. Reinforcement Learning für die Optimierung von Versicherungsprämien eingesetzt.
Es wird zwar ein domänenspezifisches Modell gewählt, das aber ebenfalls eine aktuelle Marktsituation als Modell und Preise als Aktionen verwendet.
Dabei werden zwei Rewardfunktionen vorgeschlagen, wobei die eine nur den Gewinn bewertet, und die andere zusätzlich auch Kundenbindung.
Das genutzte RL-Verfahren heißt VQQL.
Es diskretisiert erst Zustands- und Aktionsraum, und wendet anschließtend Tabular-Q-Learning an.
Bei dieser Forschungsarbeit wurde mit einer großenn spanischen Versicherungsgesellschaft zusammengearbeitet und deren Echtweltdaten verwendet.

Mit einem sehr ähnlichen Marktmodell wie bei Schlosser und Boissier haben Kastius und Schlosser \cite{Kastius2022} Reinforcement Learning eingesetzt, um unter Wettbewerbsbedingungen die Preisstrategie zu optimieren.
Untersucht wurde eine \textit{Linear Economy} im Duopol und Oligopol.
Als RL-Verfahren wurden neben dem etablierten Deep-Q-Networks (mit diskretem Aktionsraum) das neue und beliebte Soft-Actor-Critic-Verfahren ausprobiert.
Dabei schnitt Soft Actor Critic erheblich besser ab, aber die DQNs konnten die Aufgabe auch bewältigen.
Es wurde weiterhin festgestellt, dass im Duopol gelegentlich Kartelle gebildet werden, wenn die Kaufbereitschaft der Kunden bei höheren Preisen nicht begrenzt wird.

Während es bisher kaum detaillierte Analysen der aktuelleren RL-Verfahren im Bereich des Dynamic Pricing gibt, so wurden die Algorithmen PPO und SAC auf anderen interessanten Problemstellungen verglichen.
Larsen et al. verglichen PPO, DDPG, TD3 und SAC in der Stable-Baselines-Implementierung bei autonomer Schifffahrt.
Aufgabe ist, mit einem Schiff Routen zwischen Häfen zu fahren, ohne dabei mit Hindernissen zu kollidieren.
Zur Analyse wurden verschiedene Situationen getestet, bei denen die Algorithmen jeweils unterschiedlich gut abschnitten.
Nur Proximal Policy Optimization konnte jede Umgebung besser lösen als alle anderen Algorithmen und hat damit im Vergleich mit Abstand am besten abgeschnitten.

Im Bereich der unternehmensnahen Software haben Alves et al \cite{10.1007/978-3-030-87897-9_21} die Algorithmen A2C, DDPG, TD3, PPO und SAC auf einem Problem verglichen, bei dem es darum geht, Lagerstände entlang einer Lieferkette zu optimieren.
Diese Lieferkette ist von Unsicherheiten  bei Bedarf- und Vorlaufzeiten geprägt.
Im Ergebnis übertrafen SAC und PPO die anderen Algorithmen, während sie selbst ähnlich gut abschnitten.