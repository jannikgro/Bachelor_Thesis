\section{MDPs lösen: Zustands- und Aktionswerte}
Gegeben sei ein Markov-Entscheidungsprozess $(\mathcal{S}, \mathcal{A}, r, p)$ (kurz MDP).
Dabei ist $\mathcal{S}$ der Zustands- und $\mathcal{A}$ der Aktionsraum.
$r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ ist eine Zufallsvariable, die die Belohnung angibt.
Weiterhin ist $p: \mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$ die Dichtefunktion der Zustandsübergänge.
Zentraler Begriff beim Lösen eines MDPs die sogenannte Policyfunktion $\pi$.
Sie kann deterministisch sein und als $\pi: \mathcal{S} \rightarrow \mathcal{A}$ formalisiert werden.
Weiterhin können Policies stochastisch sein, sie sind dann eine Zähl- oder Wahrscheinlichkeitsdichtefunktion der Form $\pi: \mathcal{S}\times\mathcal{A}\rightarrow [0, 1]$.
Mit einem Diskontierungsfaktor $\gamma \in [0,1]$ werden nun Zustandswerte in Bezug auf eine Policy als
\begin{equation}
    V_\pi: \mathcal{S} \rightarrow \mathbb{R}; s \mapsto \sum_{a\in\mathcal{A}}\pi(s, a) Q(s, a)
\end{equation}
definiert.
Dabei wird als Abkürzung bereits die Aktionswertefunktion verwendet, die ihrerseits mithilfe der Zustandswerte als
\begin{equation}
    Q_\pi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}; (s, a) \mapsto \mathbb{E}\left[r(s,a)\right]+\gamma \sum_{s'\in\mathcal{S}}{p(s', s, a) V(s')}
\end{equation}
definiert ist.
Endzustände erhalten den Zustandswert 0.
Ziel ist im Allgemeinen, eine Policyfunktion zu finden, die den Zustandswert des ersten Zustands maximiert.
Nach dem Policy-Improvement-Theorem bedeutet das, dass auch die Zustandswerte aller anderen (erreichbaren) Zustände maximal (unter allen Policies) sind.

\section{Exakte Lösungen für das Dynamic Pricing}
Ein klassisches Verfahren für das Lösen von Markov-Entscheidungsprozessen ist die dynamische Programmierung.
Das setzt voraus, dass sowohl Zustands- als auch Aktionsraum endlich und die Übergangswahrscheinlichkeiten $p$ sowie Belohnungen $r$ bekannt sind.
Anschließend wird eine angenäherte Funktion für die Zustandswerte $V$ als Array gespeichert und mit beliebigen Werten initialisiert.
Dann werden rundenweise für alle Zustände $s\in\mathcal{S}$ ihre Einträge in dieser Tabelle mithilfe der Bellman-Gleichung
\begin{equation}
    V(s) \leftarrow \max_{a\in\mathcal{A}}{\left(\mathbb{E}[r(s, a)] + \sum_{s'\in\mathcal{S}}{p(s', s, a) V(s')}\right)}
\end{equation}
als Berechnungsvorschrift aktualisiert.
Unabhängig davon, ob diese Berechnung in-place oder mit zwei alternierenden Arrays ausgeführt wird, konvergiert sie zur Zustandswertefunktion einer optimalen Policy \cite{Sutton1998}.
Die optimale Policy ist dann die folgende deterministische, sogenannte Greedy-Policy:
\begin{equation}
    \pi: s \mapsto \argmax_{a\in\mathcal{A}}{\left(\mathbb{E}[r(s, a)] + \sum_{s'\in\mathcal{S}}{p(s', s, a) V(s')}\right)}
\end{equation}
Dieses Verfahren ist mit analogen Formeln auf Grundlage der Aktionswerte möglich.
Dann wird mehr Speicher benötigt, aber die Aktionsauswahl benötigt keine weitere Rechnungen als die Bestimmung der Aktion mit maximalem Wert.

Bei der Definition dieses Marktes ist die Lösung mittels dynamischer Programmierung sogar noch einfacher, weil die Episodenlänge begrenzt ist und es einen Schrittzähler gibt.
Dann wird zwischen dem Schrittzähler und dem (restlichen) Zustand unterschieden und eine Tabelle $V$ erstellt, die mit Zustand und Schrittzähler adressiert wird.
Die Endzustände werden mit $0$ initialisiert und die anderen Zustände dieser leicht abgewandelten Bellman-Gleichung vom Ende bis zum Anfang einer Episode befüllt:
\begin{equation}
    V(s, t) \leftarrow \max_{a\in\mathcal{A}}{\left(\mathbb{E}[r(s, a)] + \sum_{s'\in\mathcal{S}}{p(s', s, a) V(s', t+1)}\right)}
\end{equation}
Hier muss nicht einmal Konvergenz abgewartet werden, sondern die Tabelle ist nach dem Befüllen mit den exakten Zustandswerten befüllt, und die optimale Policy ist wieder die Greedy-Policy.

Dynamische Programmierung wurde bereits erfolgreich für dynamische Preisgestaltung eingesetzt [Rainer], hat allerdings starke Forderungen, die in der Praxis schwer erfüllbar sind.
Erstens müssen die Wahrscheinlichkeitsverteilungen der Belohnungen und Zustandsübergänge bekannt sein.
Das ist für die Realität nicht möglich und bereitet auch in dem in dieser Arbeit untersuchten Markt Probleme.
So könnte man zwar die explizite Definition des Marktes zur Verfügung haben, allerdings sind die Wahrscheinlichkeiten nirgends explizit aufgeschrieben, sondern die Übergangswahrscheinlichkeiten werden durch die Simulation induziert.
Sie explizit zu bekommen, ist möglich, erfordert aber eine umfangreiche mathematische Analyse.

Dieses Problem lässt sich allerdings noch im Rahmen abgewandelter DP-Verfahren lösen.
Grundidee dieser Monte-Carlo-Verfahren ist es, eine Tabelle für die Schätzung der Aktionswerte zu speichern und damit eine Greedy-Policy zu erstellen.
Es werden nach und nach Episoden generiert und mit den empirischen Daten die Schätzung der Aktionswerte verbessert.
In jedem Schritt wird dabei mit einer Wahrscheinlichkeit von $1-\varepsilon$ dieser Greedy-Policy gefolgt, allerdings mit einer Wahrscheinlichkeit von $\varepsilon$ eine zufällige Aktion ausgewählt.
Tatsächlich lässt sich zeigen, dass dieses Verfahren in der Asymptotik eine optimale Policy liefert, wenn $\varepsilon$ gegen null konvergiert, aber stets größer als null ist. \cite{Sutton1998}

\section{Approximation als Ausweg bei zu großem Zustands- und Aktionsraum}
Doch auch dieses Verfahren kann auf den beschriebenen Markt kaum angewandt werden.
In jedem Fall müsste die Formulierung mit dem diskreten Aktionsraum gewählt werden.
Hindernis ist aber auch hier der große Zustands- und Aktionsraum: soll das Monte-Carlo-Verfahren angewandt werden, so ist dafür eine Tabelle der Größe $|\mathcal{S} \times \mathcal{A}|$ erforderlich.
Bei den in Tabelle \ref{tab:default_parameters} genannten Standardwerten müsste die Tabelle mindestens\footnote{
    Diese Zahl ist eine untere Schranke, da für die Anzahl der Produkte in Zirkulation nur $5 m_{lager}=500$ veranschlagt wurde.
    Wenn mehr Neuprodukte verkauft als Produkte zurückverkauft und weggeworfen werden, so ist hier ein noch größeres Array nötig.
} $50 \cdot 500 \cdot 100 \cdot 10^3 \cdot 100 \cdot 10^3 = 2.5 \cdot 10^{14}$ Einträge haben.
Obwohl dieser Wert bereits jeden verfügbaren Speicher sprengt, ist die Anzahl der unterschiedlichen Preisstufen mit 10 für realistische Verhältnisse noch viel zu niedrig gewählt.
Tatsächlich liegt der benötigte Speicher in $\mathcal{O}(p_{max}^6)$, was für eine Skalierung selbst bei ansonstem kleineren Zustandsraum unmöglich macht.
Weil dies sogar nur die Betrachtung des Speichers war -- bis Konvergenz erreicht ist, müsste jeder Zustand sehr oft durchlaufen werden -- muss geschlussfolgert werden, dass eine Lösung, die für alle Zustände die Zustandswerte ausrechnen möchte, rechentechnisch nicht machbar ist.

Approximationen sind also unumgänglich.
Sie beruhen auf der Einsicht, dass der Zustandsraum zwar sehr groß ist, aber viele Zustände sich sehr ähnlich sind.
Dadurch können Modelle mit deutlich weniger Parametern als Zuständen diese gewünschten Funktionen  mit für praktische Zwecke ausreichender Genauigkeit annähern.
Alle folgenden Verfahren nutzen sogenannte künstliche neuronale Netze, die am weitesten verbreitete Technologie um komplexe nichtlineare Funktionen zu approximieren.

\section{Policy-Gradient-Verfahren}
Vom Policy-Gradient-Theorem über A2C zu PPO

\section{Q-Learning-Verfahren im stetigen Aktionsraum}
Schreib etwas zu DDPG und TD3

\section{Soft Actor Critic}
Nicht nur die Library aufrufen!

\section{Bestimmung der richtigen Hyperparameters}
Schreib etwas zu sinnvollen Standardwerten und vielleicht zu Grid-Search.
