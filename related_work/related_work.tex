\section{MDPs lösen: Zustands- und Aktionswerte}
Gegeben sei ein Markov-Entscheidungsprozess $(\mathcal{S}, \mathcal{A}, r, p)$ (kurz MDP).
Dabei ist $\mathcal{S}$ der Zustands- und $\mathcal{A}$ der Aktionsraum.
$r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ ist eine Zufallsvariable, die die Belohnung angibt.
Weiterhin ist $p: \mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$ die Dichtefunktion der Zustandsübergänge.
Zentraler Begriff beim Lösen eines MDPs die sogenannte Policyfunktion $\pi$.
Sie kann deterministisch sein und als $\pi: \mathcal{S} \rightarrow \mathcal{A}$ formalisiert werden.
Weiterhin können Policies stochastisch sein, sie sind dann eine Zähl- oder Wahrscheinlichkeitsdichtefunktion der Form $\pi: \mathcal{S}\times\mathcal{A}\rightarrow [0, 1]$.
Mit einem Diskontierungsfaktor $\gamma \in [0,1]$ werden nun Zustandswerte in Bezug auf eine Policy als
\begin{equation}
    V_\pi: \mathcal{S} \rightarrow \mathbb{R}; s \mapsto \sum_{a\in\mathcal{A}}\pi(s, a) Q(s, a)
\end{equation}
definiert.
Dabei wird als Abkürzung bereits die Aktionswertefunktion verwendet, die ihrerseits mithilfe der Zustandswerte als
\begin{equation}
    Q_\pi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}; (s, a) \mapsto \mathbb{E}\left[r(s,a)\right]+\gamma \sum_{s'\in\mathcal{S}}{p(s', s, a) V(s')}
\end{equation}
definiert ist.
Endzustände erhalten den Zustandswert 0.
Ziel ist im Allgemeinen, eine Policyfunktion zu finden, die den Zustandswert des ersten Zustands maximiert.
Nach dem Policy-Improvement-Theorem bedeutet das, dass auch die Zustandswerte aller anderen (erreichbaren) Zustände maximal (unter allen Policies) sind.

\section{Exakte Lösungen für das Dynamic Pricing}
Ein klassisches Verfahren für das Lösen von Markov-Entscheidungsprozessen ist die dynamische Programmierung.
Das setzt voraus, dass sowohl Zustands- als auch Aktionsraum endlich und die Übergangswahrscheinlichkeiten $p$ sowie Belohnungen $r$ bekannt sind.
Anschließend wird eine angenäherte Funktion für die Zustandswerte $V^*$ als Array gespeichert und mit beliebigen Werten initialisiert.
Dann werden rundenweise für alle Zustände $s\in\mathcal{S}$ ihre Einträge in dieser Tabelle mithilfe der Bellman-Gleichung
\begin{equation}
    V^*(s) \leftarrow \max_{a\in\mathcal{A}}{\left(\mathbb{E}[r(s, a)] + \sum_{s'\in\mathcal{S}}{p(s', s, a) V^*(s')}\right)}
\end{equation}
als Berechnungsvorschrift aktualisiert.
Unabhängig davon, ob diese Berechnung in-place oder mit zwei alternierenden Arrays ausgeführt wird, konvergiert sie zur Zustandswertefunktion einer optimalen Policy \cite{Sutton1998}.
Die optimale Policy ist dann die folgende deterministische, sogenannte Greedy-Policy:
\begin{equation}
    \pi: s \mapsto \argmax_{a\in\mathcal{A}}{\left(\mathbb{E}[r(s, a)] + \sum_{s'\in\mathcal{S}}{p(s', s, a) V(s')}\right)}
\end{equation}

Bei der Definition dieses Marktes ist die Lösung mittels dynamischer Programmierung sogar noch einfacher, weil die Episodenlänge begrenzt ist und es einen Schrittzähler gibt, dessen Wert während einer Episode strikt monoton steigt.
Dann wird zwischen dem Schrittzähler und dem (restlichen) Zustand unterschieden und eine Tabelle $V^*$ erstellt, die mit Zustand und Schrittzähler adressiert wird.
Die Endzustände werden mit $0$ initialisiert und die anderen Zustände dieser leicht abgewandelten Bellman-Gleichung vom Ende bis zum Anfang befüllt:
\begin{equation}
    V^*(s, t) \leftarrow \max_{a\in\mathcal{A}}{\left(\mathbb{E}[r(s, a)] + \sum_{s'\in\mathcal{S}}{p(s', s, a) V^*(s', t+1)}\right)}
\end{equation}
Hier muss nicht einmal Konvergenz abgewartet werden, sondern die Tabelle ist nach dem Befüllen mit den exakten Zustandswerten befüllt, und die optimale Policy ist wieder die Greedy-Policy.

Dynamische Programmierung wurde bereits erfolgreich für dynamische Preisgestaltung eingesetzt [Rainer], hat allerdings starke Forderungen, die in der Praxis schwer erfüllbar sind.

\section{Policy-Gradient-Verfahren}
Vom Policy-Gradient-Theorem über A2C zu PPO

\section{Q-Learning-Verfahren im stetigen Aktionsraum}
Schreib etwas zu DDPG und TD3

\section{Soft Actor Critic}
Nicht nur die Library aufrufen!

\section{Bestimmung der richtigen Hyperparameters}
Schreib etwas zu sinnvollen Standardwerten und vielleicht zu Grid-Search.
