\section{MDPs lösen: Zustands- und Aktionswerte}
Gegeben sei ein Markov-Entscheidungsprozess $(\mathcal{S}, \mathcal{A}, r, p)$ (kurz MDP).
Dabei ist $\mathcal{S}$ der Zustands- und $\mathcal{A}$ der Aktionsraum.
$r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ ist eine Zufallsvariable, die die Belohnung angibt.
Weiterhin ist $p: \mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$ die Dichtefunktion der Zustandsübergänge.
Zentraler Begriff beim Lösen eines MDPs die sogenannte Policyfunktion $\pi$.
Sie kann deterministisch sein und als $\pi: \mathcal{S} \rightarrow \mathcal{A}$ formalisiert werden.
Weiterhin können Policies stochastisch sein, sie sind dann eine Zähl- oder Wahrscheinlichkeitsdichtefunktion der Form $\pi: \mathcal{S}\times\mathcal{A}\rightarrow [0, 1]$.
Mit einem Diskontierungsfaktor $\gamma \in [0,1]$ werden nun Zustandswerte in Bezug auf eine Policy als
\begin{equation}
    V_\pi: \mathcal{S} \rightarrow \mathbb{R}; s \mapsto \sum_{a\in\mathcal{A}}\pi(s, a) Q(s, a)
\end{equation}
definiert.
Dabei wird als Abkürzung bereits die Aktionswertefunktion verwendet, die ihrerseits mithilfe der Zustandswerte als
\begin{equation}
    Q_\pi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}; (s, a) \mapsto \mathbb{E}\left[r(s,a)\right]+\gamma \sum_{s'\in\mathcal{S}}{p\left(s', s, a\right) V\left(s'\right)}
\end{equation}
definiert ist.
Endzustände erhalten den Zustandswert 0.
Ziel ist im Allgemeinen, eine Policyfunktion zu finden, die den Zustandswert des ersten Zustands maximiert.
Nach dem Policy-Improvement-Theorem bedeutet das, dass auch die Zustandswerte aller anderen (erreichbaren) Zustände maximal (unter allen Policies) sind.

\section{Exakte Lösungen für das Dynamic Pricing}
\label{section:dp}
Ein klassisches Verfahren für das Lösen von Markov-Entscheidungsprozessen ist die dynamische Programmierung.
Das setzt voraus, dass sowohl Zustands- als auch Aktionsraum endlich und die Übergangswahrscheinlichkeiten $p$ sowie Belohnungen $r$ bekannt sind.
Anschließend wird eine angenäherte Funktion für die Zustandswerte $V$ als Array gespeichert und mit beliebigen Werten initialisiert.
Dann werden rundenweise für alle Zustände $s\in\mathcal{S}$ ihre Einträge in dieser Tabelle mithilfe der Bellman-Gleichung
\begin{equation}
    V(s) \leftarrow \max_{a\in\mathcal{A}}{\left(\mathbb{E}[r(s, a)] + \sum_{s'\in\mathcal{S}}{p\left(s', s, a\right) V\left(s'\right)}\right)}
\end{equation}
als Berechnungsvorschrift aktualisiert.
Unabhängig davon, ob diese Berechnung in-place oder mit zwei alternierenden Arrays ausgeführt wird, konvergiert sie zur Zustandswertefunktion einer optimalen Policy \cite{Sutton1998}.
Die optimale Policy ist dann die folgende deterministische, sogenannte Greedy-Policy:
\begin{equation}
    \pi: s \mapsto \argmax_{a\in\mathcal{A}}{\left(\mathbb{E}[r(s, a)] + \sum_{s'\in\mathcal{S}}{p\left(s', s, a\right) V\left(s'\right)}\right)}
\end{equation}
Dieses Verfahren ist mit analogen Formeln auf Grundlage der Aktionswerte möglich.
Dann wird mehr Speicher benötigt, aber die Aktionsauswahl benötigt keine weitere Rechnungen als die Bestimmung der Aktion mit maximalem Wert.

Bei der Definition dieses Marktes ist die Lösung mittels dynamischer Programmierung sogar noch einfacher, weil die Episodenlänge begrenzt ist und es einen Schrittzähler gibt.
Dann wird zwischen dem Schrittzähler und dem (restlichen) Zustand unterschieden und eine Tabelle $V$ erstellt, die mit Zustand und Schrittzähler adressiert wird.
Die Endzustände werden mit $0$ initialisiert und die anderen Zustände dieser leicht abgewandelten Bellman-Gleichung vom Ende bis zum Anfang einer Episode befüllt:
\begin{equation}
    V(s, t) \leftarrow \max_{a\in\mathcal{A}}{\left(\mathbb{E}[r(s, a)] + \sum_{s'\in\mathcal{S}}{p\left(s', s, a\right) V\left(s', t+1\right)}\right)}
\end{equation}
Hier muss nicht einmal Konvergenz abgewartet werden, sondern die Tabelle ist nach dem Befüllen mit den exakten Zustandswerten befüllt, und die optimale Policy ist wieder die Greedy-Policy.

Dynamische Programmierung wurde bereits erfolgreich für dynamische Preisgestaltung eingesetzt \cite{10.1145/3219819.3219833}, hat allerdings starke Forderungen, die in der Praxis schwer erfüllbar sind.
Erstens müssen die Wahrscheinlichkeitsverteilungen der Belohnungen und Zustandsübergänge bekannt sein.
Das ist für die Realität nicht möglich und bereitet auch in dem in dieser Arbeit untersuchten Markt Probleme.
So könnte man zwar die explizite Definition des Marktes zur Verfügung haben, allerdings sind die Wahrscheinlichkeiten nirgends explizit aufgeschrieben, sondern die Übergangswahrscheinlichkeiten werden durch die Simulation induziert.
Sie explizit zu bekommen, ist möglich, erfordert aber eine umfangreiche mathematische Analyse.

Dieses Problem lässt sich allerdings noch im Rahmen abgewandelter DP-Verfahren lösen.
Grundidee dieser Monte-Carlo-Verfahren ist es, eine Tabelle für die Schätzung der Aktionswerte zu speichern und damit eine Greedy-Policy zu erstellen.
Es werden nach und nach Episoden generiert und mit den empirischen Daten die Schätzung der Aktionswerte verbessert.
In jedem Schritt wird dabei mit einer Wahrscheinlichkeit von $1-\varepsilon$ dieser Greedy-Policy gefolgt, allerdings mit einer Wahrscheinlichkeit von $\varepsilon$ eine zufällige Aktion ausgewählt.
Tatsächlich lässt sich zeigen, dass dieses Verfahren in der Asymptotik eine optimale Policy liefert, wenn $\varepsilon$ gegen null konvergiert, aber stets größer als null ist. \cite{Sutton1998}

\section{Approximation als Ausweg bei zu großem Zustands- und Aktionsraum}
Doch auch dieses Verfahren kann auf den beschriebenen Markt kaum angewandt werden.
In jedem Fall müsste die Formulierung mit dem diskreten Aktionsraum gewählt werden.
Hindernis ist aber auch hier der große Zustands- und Aktionsraum: soll das Monte-Carlo-Verfahren angewandt werden, so ist dafür eine Tabelle der Größe $|\mathcal{S} \times \mathcal{A}|$ erforderlich.
Bei den in Tabelle \ref{tab:default_parameters} genannten Standardwerten müsste die Tabelle mindestens\footnote{
    Diese Zahl ist eine untere Schranke, da für die Anzahl der Produkte in Zirkulation nur $5 m_{lager}=500$ veranschlagt wurde.
    Wenn mehr Neuprodukte verkauft als Produkte zurückverkauft und weggeworfen werden, so ist hier ein noch größeres Array nötig.
} $50 \cdot 500 \cdot 100 \cdot 10^3 \cdot 100 \cdot 10^3 = 2.5 \cdot 10^{14}$ Einträge haben.
Obwohl dieser Wert bereits jeden verfügbaren Speicher sprengt, ist die Anzahl der unterschiedlichen Preisstufen mit 10 für realistische Verhältnisse noch viel zu niedrig gewählt.
Tatsächlich liegt der benötigte Speicher in $\mathcal{O}\left(p_{max}^6\right)$, was eine Skalierung selbst bei ansonstem kleineren Zustandsraum unmöglich macht.
Weil dies sogar nur die Betrachtung des Speichers war -- bis Konvergenz erreicht ist, müsste jeder Zustand sehr oft durchlaufen werden -- muss geschlussfolgert werden, dass eine Lösung, die für alle Zustände die Zustandswerte ausrechnen möchte, rechentechnisch nicht machbar ist.

Approximationen sind also unumgänglich.
Sie beruhen auf der Einsicht, dass der Zustandsraum zwar sehr groß ist, aber viele Zustände sich sehr ähnlich sind.
Dadurch können Modelle mit deutlich weniger Parametern als Zuständen diese gewünschten Funktionen  mit für praktische Zwecke ausreichender Genauigkeit annähern.
Alle folgenden Verfahren nutzen sogenannte künstliche neuronale Netze, die am weitesten verbreitete Technologie um komplexe nichtlineare Funktionen zu approximieren.

\section{Q-Learning-Verfahren im stetigen Aktionsraum}
Ein weit verbreitetes Verfahren zum Lösen von Markov-Entscheidungsprozessen ist das sogenannte Q-Learning.
Die Idee ist hierbei, wie in dem in Abschnitt \ref{section:dp} beschriebenen Monte-Carlo-Verfahren, Aktionswerte einer optimalen Policy zu schätzen.
Dazu wird als Berechnungsvorschrift die Bellman-Gleichung verwendet.
Allerdings werden nicht unbedingt vollständige Episoden betrachtet, sondern der Aktionswert nach dem Folgezustand ergibt sich mittels der momentanen Schätzfunktion.
Diese Technik, zur Verbesserung von Schätzwerten rekursiv bisherige Schätzwerte zu verwenden, wird Bootstrapping genannt und gilt als wichtige Technologie angesehen. \cite{Sutton1998}
Nach dem Training wird wieder die Greedy-Policy verwendet.
Während des Trainings wird eine $\varepsilon$-Soft-Policy genutzt, um für ausreichende Exploration zu sorgen.

Dieses Verfahren lässt sich unmittelbar mit Tabellen umsetzen.
Die Umsetzung dieses Verfahrens mit neuronalen Netzen führt zu dem erfolgreichen >>Deep-Q-Learning<<.
Solche >>Deep-Q-Networks<< (DQNs) sind zunächst für diskrete Aktionsräume entworfen worden.
Die Schätzung der Aktionswertefunktion wird dabei so umgesetzt, dass an den Input des Netzes nur den Zustand angelegt wird und dann in einer Berechnung auf so viele Ausgabeneuronen abgebildet wird, wie es Aktionen gibt.
So erhält man bei nur einem der aufwendigen Läufe durch das neuronale Netz bereits alle Aktionswerte auf einmal.
Die Wahl des Maximums und des Argmax sind dann unmittelbar möglich.
Das Training findet iterativ statt, indem die bisherigen Schätzwerte des Netzes mit Zustandsübergängen $(s, a, r, s')$ aus der gesammelten Erfahrung $\mathcal{D}$ in Richtung des Zielwertes $r+\gamma \max_{a'\in\mathcal{A}}{Q(s',a')}$ bewegt werden.
In den heutigen Deep-Learning-Frameworks wie PyTorch oder Tensorflow wird üblicherweise mit einer Verlustfunktion gearbeitet, die dann automatisch differenziert wird. [Verweis]
Mit einem Optimizer wie Stochastic Gradient Descent [Paper] oder Adam [Paper] wird dieser Verlust dann mithilfe des Gradienten minimiert.
Diese Verlustfunktion beim Deep-Q-Learning lautet für eine Schätzfunktion der Aktionswerte mit den Parametern $\psi$
\begin{equation}
    L(\psi) = \mathbb{E}_{(s, a, r, s')\sim\mathcal{D}}\left[\left(Q_\psi(s,a)-\left(r+\gamma \max_{a'\in\mathcal{A}}{Q_\psi(s',a')}\right)\right)^2\right]
\end{equation}
Erfolgreiche Q-Learning-Umsetzungen verwenden einen Experiencebuffer, in dem die Erfahrung $\mathcal{D}$ gespeichert wird.
Weiterhin speichern sie zwei Netze: neben dem, das trainiert wird, auch ein sogenanntes Zielnetz, was in der Schätzung des Zielwertes verwendet wird.
Das verhindert ungewollte Oszillationen und Instabilität. [Paper]
Zahlreiche weitere Techniken zur Verbesserung von Deep-Q-Networks sind verbreitet. [Paper]
Für die gleich folgenden Verfahren ist insbesondere die Erkenntnis wichtig, dass Q-Learning-Verfahren zur systematischen Überschätzung der Aktionswerte führt. [Paper]
Die Erklärung dahinter ist einfach zu verstehen.
Angenommen ein Zustand hat zwei Aktionen, die beide in einer optimalen Policy den tatsächlichen Zustandswert 0 haben (das ist ein Erwartungswert).
Nun stehen aber nur zwei unabhängige, standardnormalverteilte Schätzwerte dieser Aktionswerte zur Verfügung.
Dass das Maximum der beiden Zufallsvariablen kleiner-gleich null ist, hat nun eine Wahrscheinlichkeit von nur $(1/2)^2=1/4$.
Es lässt sich weiterhin zeigen, dass die Verteilung des Maximums einen Erwartungswert von etwa $0.56$ hat. [Anhang]
Um diese systematische Verzerrung zu umgehen, werden für Auswahl der Aktion und die Generierung des Schätzwertes unterschiedliche Netze herangezogen.

Die Verallgemeinerung auf stetige Aktionsräume ist bei Deep-Q-Networks nicht ohne einige grundlegende Designänderungen möglich.
Das dabei entstehende Verfahren heißt Deep Deterministic Policy Gradient (DDPG) und wurde 2014 publiziert. \cite{10.5555/3044805.3044850}
Ein neuronales Netz mit den Parametern $\psi$ berechnet die Aktionswerteschätzfunktion $Q_\psi: \mathcal{S} \times \mathcal{A}\rightarrow \mathbb{R}$ (jetzt muss die Aktion mit in die Eingabe).
Dabei ist die Bestimmung des Maximums kaum möglich: für neuronale Netze ist keine analytische Lösung möglich und numerische Verfahren sind viel zu aufwendig für die vielen benötigten Trainingsschritte.
Ein weiteres neuronales Netz mit den Parametern $\phi$ berechnet explizit die deterministische Policyfunktion $\pi_\phi: \mathcal{S} \rightarrow \mathcal{A}$.
Das Training des Q-Netzes findet wie beim DQN statt, außer dass für den Zielwert die Policyfunktion verwendet wird.
Die dazugehörige Verlustfunktion lautet 
\begin{equation}
    L(\psi) = \mathbb{E}_{(s, a, r, s')\sim\mathcal{D}}\left[\left(Q_\psi(s,a)-\left(r+\gamma Q_{\psi'}(s',\pi_\phi(s'))\right)\right)^2\right]
\end{equation}
Dabei wird bei DDPG wird ein Zielnetz mit Parametern $\psi'$ für die Schätzung der Policyfunktion verwendet, dieses allerdings nach jedem Optimierungsschritt mit dem Hyperparameter $\rho \in [0,1]$ und der Polyak-Mittelung
\begin{equation}
    \psi' \leftarrow (1 - \rho) \psi' + \rho \psi
\end{equation}
aktualisert.
Zur Optimierung der Policyfunktion in dem Sinne, dass sie für einen Zustand möglichst das Argmax berechnet, kann Gradientenanstieg verwendet werden.
Das wird dadurch möglich, dass beide neuronale Netze differenzierbar sind und der Aktionsraum stetig ist.
So kann der Gradient
\begin{equation}
    \nabla_\phi \mathbb{E}_{s\sim\mathcal{D}}\left[Q_\psi(s, \pi_\phi(s))\right]
\end{equation}
durch die automatische Differenzierung des Deep-Learning-Frameworks berechnet werden, und ein Optimierungsschritt in die Richtung des größten Anstiegs vorgenommen werden.

Nachfolgende Analysen haben ergeben, dass die systematische Überschätzung der Funktionswerte bei DDPG sich besonders stark auf die Leistung auswirkt. \cite{DBLP:journals/corr/abs-1802-09477}
In diesem Paper wurden drei Verbesserungen für den DDPG-Algorithmus vorgeschlagen:
\begin{enumerate}
    \item Es werden zwei seperate Netze für die Aktionswertefunktion gehalten.
    Für die Berechnung des Zielwertes wird immer das Minimum von beiden genommen.
    \item Weil die Q-Netze manchmal kleinere Ausbuchtungen haben, die vom Gradientenanstieg ausgenutzt werden, wird bei der Berechnung des Zielwertes der Aktionsauswahl durch das Policynetz normalverteiltes Rauschen angefügt.
    Die Aktionen werden dabei selbstverständlich auf den Aktionsraum geclippt.
    \item Im Gegensatz zu vielen DDPG-Implementierungen wird das Policy-Netz nur alle zwei Q-Netz-Updates trainiert.
\end{enumerate}
Der entstehende Algorithmus wird als TD3 bezeichnet und hat bei vielen Tests durch bessere Performance überzeugt.

\section{Policy-Gradient-Verfahren}
Vom Policy-Gradient-Theorem über A2C zu PPO

\section{Soft Actor Critic}
Nicht nur die Library aufrufen!

\section{Bestimmung der richtigen Hyperparameters}
Schreib etwas zu sinnvollen Standardwerten und vielleicht zu Grid-Search.
